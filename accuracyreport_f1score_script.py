# -*- coding: utf-8 -*-
"""subtaskA_baselines_f1score.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ufXAMiUis29fMWPtAFRMiHYDdQMKTT6C
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install transformers

import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sklearn.metrics import f1_score, classification_report
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print(device)

!pip install transformers==2.8.0

# print(transformers.__version__)

def tokenize_and_pad_text(text, max_length):
    tokens = tokenizer.tokenize(text)
    if len(tokens) > max_length - 2:  # for [CLS] and [SEP] tokens
        tokens = tokens[:max_length - 2]

    # Add special tokens ([CLS] and [SEP]) and pad to max_length
    input_ids = tokenizer.convert_tokens_to_ids(['[CLS]'] + tokens + ['[SEP]'])
    input_ids += [0] * (max_length - len(input_ids))  # Pad with zeros
    return input_ids

max_sequence_length = 512

from torch.utils.data import DataLoader, TensorDataset

batch_size = 8

model_name = "roberta-base"
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)
model.to(device)
model.load_state_dict(torch.load('/content/drive/MyDrive/subtaskA_mono_robertabase.pth'), strict=False)
model.eval()

mono_test_data = pd.read_json('/content/drive/MyDrive/subtaskA_dev_monolingual.jsonl', lines=True)

max_sequence_length = 512
tokenizer = AutoTokenizer.from_pretrained(model_name)
mono_test_data['tokenized_text'] = mono_test_data['text'].apply(lambda x: tokenize_and_pad_text(x, max_sequence_length))

test_inputs = torch.tensor(mono_test_data['tokenized_text'].values.tolist())
test_labels = mono_test_data['label'].values

!transformer

def evaluate(model, dataloader):
    model.eval()
    all_predictions = []

    with torch.no_grad():
        for batch in dataloader:
            batch_inputs, batch_labels = batch
            batch_inputs = batch_inputs.to(device)
            batch_labels = batch_labels.to(device)
            outputs = model(input_ids=batch_inputs)
            logits = outputs.logits
            _, predicted = torch.max(logits, dim=1)
            all_predictions.extend(predicted.cpu().numpy())

    return all_predictions

test_dataset = TensorDataset(test_inputs, torch.tensor(test_labels))
test_dataloader = DataLoader(test_dataset, batch_size=batch_size)

test_predictions = evaluate(model, test_dataloader)

f1 = f1_score(test_labels, test_predictions)
print(f"F1 Score: {f1:.4f}")

report = classification_report(test_labels, test_predictions, target_names=['Class 0', 'Class 1'])
print(report)